import os
import time
import re
import feedparser
import requests
from bs4 import BeautifulSoup
from supabase import create_client
from telegram.ext import Application
from datetime import datetime
from dateutil import parser as date_parser
import html

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ---
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNELS = ["@finanosint", "@time_n_John"]

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
app = Application.builder().token(TELEGRAM_TOKEN).build()

# --- –§–∏–ª—å—Ç—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ---
FILTERS = {
    "SVO": [
        r"\bsvo\b", r"\b—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è\b", r"\bspecial military operation\b",
        r"\b–≤–æ–π–Ω–∞\b", r"\bwar\b", r"\bconflict\b", r"\b–∫–æ–Ω—Ñ–ª–∏–∫—Ç\b",
        r"\b–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ\b", r"\boffensive\b", r"\b–∞—Ç–∞–∫–∞\b", r"\battack\b",
        r"\b—É–¥–∞—Ä\b", r"\bstrike\b", r"\b–æ–±—Å—Ç—Ä–µ–ª\b", r"\bshelling\b",
        r"\b–¥—Ä–æ–Ω\b", r"\bdrone\b", r"\bmissile\b", r"\b—Ä–∞–∫–µ—Ç–∞\b",
        r"\b—ç—Å–∫–∞–ª–∞—Ü–∏—è\b", r"\bescalation\b", r"\b–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è\b", r"\bmobilization\b",
        r"\b—Ñ—Ä–æ–Ω—Ç\b", r"\bfrontline\b", r"\b–∑–∞—Ö–≤–∞—Ç\b", r"\bcapture\b",
        r"\b–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ\b", r"\bliberation\b", r"\b–±–æ–π\b", r"\bbattle\b",
        r"\b–ø–æ—Ç–µ—Ä–∏\b", r"\bcasualties\b", r"\b–ø–æ–≥–∏–±\b", r"\bkilled\b",
        r"\b—Ä–∞–Ω–µ–Ω\b", r"\binjured\b", r"\b–ø–ª–µ–Ω–Ω—ã–π\b", r"\bprisoner of war\b",
        r"\b–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã\b", r"\btalks\b", r"\b–ø–µ—Ä–µ–º–∏—Ä–∏–µ\b", r"\bceasefire\b",
        r"\b—Å–∞–Ω–∫—Ü–∏–∏\b", r"\bsanctions\b", r"\b–æ—Ä—É–∂–∏–µ\b", r"\bweapons\b",
        r"\b–ø–æ—Å—Ç–∞–≤–∫–∏\b", r"\bsupplies\b", r"\bhimars\b", r"\batacms\b",
        r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bminutos atr√°s\b", r"\bÂ∞èÊó∂Ââç\b"
    ],
    "Crypto": [
        r"\bbitcoin\b", r"\bbtc\b", r"\b–±–∏—Ç–∫–æ–∏–Ω\b", r"\bÊØîÁâπÂ∏Å\b",
        r"\bethereum\b", r"\beth\b", r"\b—ç—Ñ–∏—Ä\b", r"\b‰ª•Â§™Âùä\b",
        r"\bbinance coin\b", r"\bbnb\b", r"\busdt\b", r"\btether\b",
        r"\bxrp\b", r"\bripple\b", r"\bcardano\b", r"\bada\b",
        r"\bsolana\b", r"\bsol\b", r"\bdoge\b", r"\bdogecoin\b",
        r"\bavalanche\b", r"\bavax\b", r"\bpolkadot\b", r"\bdot\b",
        r"\bchainlink\b", r"\blink\b", r"\btron\b", r"\btrx\b",
        r"\bcbdc\b", r"\bcentral bank digital currency\b", r"\b—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å\b",
        r"\bdigital yuan\b", r"\beuro digital\b", r"\bdefi\b", r"\b–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã\b",
        r"\bnft\b", r"\bnon-fungible token\b", r"\bsec\b", r"\b—Ü–± —Ä—Ñ\b",
        r"\b—Ä–µ–≥—É–ª—è—Ü–∏—è\b", r"\bregulation\b", r"\b–∑–∞–ø—Ä–µ—Ç\b", r"\bban\b",
        r"\b–º–∞–π–Ω–∏–Ω–≥\b", r"\bmining\b", r"\bhalving\b", r"\b—Ö–∞–ª–≤–∏–Ω–≥\b",
        r"\b–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å\b", r"\bvolatility\b", r"\bcrash\b", r"\b–∫—Ä–∞—Ö\b",
        r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bÂàöÂàö\b", r"\bÿØŸÇÿßÿ¶ŸÇ ŸÖÿ∂ÿ™\b"
    ],
    "Pandemic": [
        r"\bpandemic\b", r"\b–ø–∞–Ω–¥–µ–º–∏—è\b", r"\bÁñ´ÊÉÖ\b", r"\bÿ¨ÿßÿ¶ÿ≠ÿ©\b",
        r"\boutbreak\b", r"\b–≤—Å–ø—ã—à–∫–∞\b", r"\b—ç–ø–∏–¥–µ–º–∏—è\b", r"\bepidemic\b",
        r"\bvirus\b", r"\b–≤–∏—Ä—É—Å\b", r"\b–≤–∏—Ä—É—Å—ã\b", r"\bÂèòÂºÇÊ†™\b",
        r"\bvaccine\b", r"\b–≤–∞–∫—Ü–∏–Ω–∞\b", r"\bÁñ´Ëãó\b", r"\bŸÑŸÇÿßÿ≠\b",
        r"\bbooster\b", r"\b–±—É—Å—Ç–µ—Ä\b", r"\b—Ä–µ–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è\b",
        r"\bquarantine\b", r"\b–∫–∞—Ä–∞–Ω—Ç–∏–Ω\b", r"\bÈöîÁ¶ª\b", r"\bÿ≠ÿ¨ÿ± ÿµÿ≠Ÿä\b",
        r"\blockdown\b", r"\b–ª–æ–∫–¥–∞—É–Ω\b", r"\bÂ∞ÅÈîÅ\b",
        r"\bmutation\b", r"\b–º—É—Ç–∞—Ü–∏—è\b", r"\bÂèòÂºÇ\b",
        r"\bstrain\b", r"\b—à—Ç–∞–º–º\b", r"\bomicron\b", r"\bdelta\b",
        r"\bbiosafety\b", r"\b–±–∏–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\b", r"\bÁîüÁâ©ÂÆâÂÖ®\b",
        r"\blab leak\b", r"\b–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —É—Ç–µ—á–∫–∞\b", r"\bÂÆûÈ™åÂÆ§Ê≥ÑÊºè\b",
        r"\bgain of function\b", r"\b—É—Å–∏–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\b",
        r"\bwho\b", r"\b–≤–æ–∑\b", r"\bcdc\b", r"\b—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä\b",
        r"\binfection rate\b", r"\b–∑–∞—Ä–∞–∑–Ω–æ—Å—Ç—å\b", r"\bÊ≠ª‰∫°Áéá\b",
        r"\bhospitalization\b", r"\b–≥–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è\b",
        r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bŸÇÿ®ŸÑ ÿ≥ÿßÿπÿßÿ™\b", r"\bÂàöÂàöÊä•Âëä\b"
    ]
}

# --- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ---
SOURCES = [
    {"name": "The Economist", "rss": "https://www.economist.com/rss/latest/rss.xml"},
    {"name": "Bloomberg", "rss": "https://feeds.bloomberg.com/markets/news.rss"},
    {"name": "RAND Corporation", "rss": "https://www.rand.org/rss.xml"},
    {"name": "CSIS", "rss": "https://www.csis.org/rss.xml"},
    {"name": "Chatham House", "rss": "https://www.chathamhouse.org/feed"},
    {"name": "Foreign Affairs", "rss": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "CFR", "rss": "https://www.cfr.org/rss.xml"},
    {"name": "BBC Future", "rss": "https://www.bbc.com/future/rss"},
    {"name": "Future Timeline", "rss": "https://futuretimeline.net/blog.rss"},
    {"name": "Carnegie Endowment", "rss": "https://carnegieendowment.org/feed"},
    {"name": "Bruegel", "rss": "https://bruegel.org/feed/"},
    {"name": "E3G", "rss": "https://e3g.org/feed/"},
    {"name": "Atlantic Council", "custom_parser": lambda: scrape_atlantic_council()},
    {"name": "Good Judgment", "custom_parser": lambda: scrape_good_judgment()},
    {"name": "Metaculus", "custom_parser": lambda: scrape_metaculus()},
    {"name": "DNI Global Trends", "custom_parser": lambda: scrape_odni()},
]

# --- –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–∫–∏ ---
COMPILED_FILTERS = {
    cat: [re.compile(p, re.IGNORECASE) for p in patterns]
    for cat, patterns in FILTERS.items()
}

# --- –§—É–Ω–∫—Ü–∏–∏ ---
def contains_keywords(text, category):
    if not text:
        return False
    return any(pattern.search(text) for pattern in COMPILED_FILTERS[category])

def classify_article(title, summary):
    text = f"{title} {summary}".lower()
    for category in ["SVO", "Crypto", "Pandemic"]:
        if contains_keywords(text, category):
            return category
    return None

def is_recent(entry, max_hours=2):
    try:
        pub = date_parser.parse(entry.published)
        now = datetime.now(pub.tzinfo)
        diff_hours = (now - pub).total_seconds() / 3600
        return diff_hours < max_hours
    except:
        return True

def article_exists(url):
    try:
        response = supabase.table("news_articles").select("id").eq("url", url).execute()
        return len(response.data) > 0
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        return False

def save_article(title, url, description, pub_date, source, category):
    try:
        supabase.table("news_articles").insert({
            "title": title,
            "url": url,
            "description": description or "",
            "pub_date": pub_date,
            "source_name": source,
            "category": category
        }).execute()
        return True
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        return False

async def async_send_to_telegram(title, url, source, category, summary=None):
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    message = f"<b>{source.upper()}</b>: {title}\n\n"
    
    if summary:
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è
        summary = summary[:500] + "..." if len(summary) > 500 else summary
        message += f"{summary}\n\n"
    
    message += f"–ò—Å—Ç–æ—á–Ω–∏–∫: <a href='{url}'>{source}</a>"
    
    for channel in CHANNELS:
        try:
            await app.bot.send_message(
                chat_id=channel,
                text=message,
                parse_mode="HTML",
                disable_web_page_preview=False
            )
            print(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {channel}: {title[:50]}...")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {channel}: {e}")

def send_to_telegram(title, url, source, category, summary=None):
    import asyncio
    asyncio.run(async_send_to_telegram(title, url, source, category, summary))

# --- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Atlantic Council ---
def scrape_atlantic_council(url="https://www.atlanticcouncil.org/blogs/ukrainealert/"):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Atlantic Council, –æ—Å–æ–±–µ–Ω–Ω–æ —Ä–∞–∑–¥–µ–ª–∞ UkraineAlert"""
    print("üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π Atlantic Council...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    try:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ HTTP –æ—à–∏–±–∫–∏
        
        # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –≤ –±–ª–æ–≥–µ UkraineAlert
        articles = []
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—â–µ–º –ø–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º —Å—Ç–∞—Ç–µ–π
        article_containers = soup.find_all('article', class_=lambda x: x and ('post' in x.lower() or 'article' in x.lower()))
        
        if not article_containers:
            # –í–∞—Ä–∏–∞–Ω—Ç 2: –ò—â–µ–º –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            article_containers = soup.select('div[class*="blog-post"], div[class*="post-"]')
        
        if not article_containers:
            # –í–∞—Ä–∏–∞–Ω—Ç 3: –ò—â–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º h2/h3 —Å —Å—Å—ã–ª–∫–∞–º–∏
            titles = soup.find_all(['h2', 'h3'])
            for title in titles:
                link = title.find('a')
                if link and '/blogs/ukrainealert/' in link.get('href', ''):
                    article_containers.append(title.parent)
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π: {len(article_containers)}")
        
        for container in article_containers[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Å–≤–µ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π
            try:
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_tag = container.find(['h1', 'h2', 'h3'], class_=lambda x: x and ('title' in x.lower() or 'headline' in x.lower()))
                if not title_tag:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    title_tag = container.select_one('h2 a, h3 a, .post-title a, .entry-title a')
                    if title_tag and title_tag.name == 'a':
                        title = title_tag.get_text(strip=True)
                        link = title_tag.get('href')
                    else:
                        continue
                else:
                    title = title_tag.get_text(strip=True)
                    link_tag = title_tag.find('a')
                    link = link_tag.get('href') if link_tag else None
                
                if not link:
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫—É
                if not link.startswith('http'):
                    if link.startswith('/'):
                        link = f"https://www.atlanticcouncil.org{link}"
                    else:
                        link = f"https://www.atlanticcouncil.org/blogs/ukrainealert/{link}"
                
                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                summary = ""
                summary_tag = container.find('p', class_=lambda x: x and ('excerpt' in x.lower() or 'summary' in x.lower() or 'description' in x.lower()))
                if not summary_tag:
                    summary_tag = container.select_one('div.entry-content p, .post-content p, .article-content p')
                
                if summary_tag:
                    summary = summary_tag.get_text(strip=True)
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ meta-—Ç–µ–≥–æ–≤ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—Ç–∞—Ç—å–∏
                if not summary:
                    try:
                        article_response = requests.get(link, headers=headers, timeout=15)
                        article_response.encoding = 'utf-8'
                        article_soup = BeautifulSoup(article_response.text, 'html.parser')
                        
                        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                        content_div = article_soup.select_one('div.entry-content, div.post-content, div.article-content, div.blog-content')
                        if content_div:
                            first_p = content_div.find('p')
                            if first_p:
                                summary = first_p.get_text(strip=True)[:300] + "..."
                    except Exception as e:
                        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏: {e}")
                
                # –ò—â–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = datetime.now().isoformat()
                date_tag = container.find('time') or container.select_one('time, .date, .published')
                if date_tag:
                    date_str = date_tag.get('datetime') or date_tag.get_text(strip=True)
                    try:
                        if date_str:
                            pub_date_obj = date_parser.parse(date_str)
                            pub_date = pub_date_obj.isoformat()
                    except Exception as e:
                        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {e}")
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é
                article = {
                    "title": html.unescape(title),
                    "url": link,
                    "summary": html.unescape(summary) if summary else "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
                    "pub_date": pub_date,
                    "source": "Atlantic Council"
                }
                articles.append(article)
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {title[:60]}...")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
                continue
        
        if not articles:
            print("‚ö†Ô∏è –°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞ –º–æ–≥–ª–∞ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è.")
            # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML –≤ –ª–æ–≥
            print(f"üîç –§—Ä–∞–≥–º–µ–Ω—Ç HTML: {response.text[:500]}...")
        
        return articles
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Atlantic Council: {e}")
        print(f"–°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code if 'response' in locals() else '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞'}")
        return []

# --- –ü–∞—Ä—Å–µ—Ä—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤ ---
def scrape_good_judgment():
    print("üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π Good Judgment...")
    url = "https://goodjudgment.com/blog"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        posts = soup.select('article.post, div.blog-post, .post-item')[:5]
        
        for post in posts:
            title_tag = post.select_one('h2, h3, .post-title, .entry-title')
            if not title_tag:
                continue
                
            title = title_tag.get_text(strip=True)
            link_tag = title_tag.find('a') or post.select_one('a.read-more, a.more-link')
            if not link_tag:
                continue
                
            link = link_tag.get('href')
            if not link.startswith('http'):
                link = "https://goodjudgment.com" + link
                
            summary_tag = post.select_one('p.excerpt, .post-excerpt, .entry-summary, p')
            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            
            pub_date = datetime.now().isoformat()
            date_tag = post.select_one('time, .date, .published')
            if date_tag:
                date_str = date_tag.get('datetime') or date_tag.get_text(strip=True)
                try:
                    pub_date = date_parser.parse(date_str).isoformat()
                except:
                    pass
            
            if article_exists(link):
                print(f"‚è© –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {title[:50]}...")
                continue
                
            category = classify_article(title, summary)
            if category:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è [{category}]: {title}")
                if save_article(title, link, summary, pub_date, "Good Judgment", category):
                    send_to_telegram(title, link, "Good Judgment", category, summary)
            else:
                print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é: {title[:50]}...")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Good Judgment: {e}")

def scrape_metaculus():
    print("üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π Metaculus...")
    url = "https://www.metaculus.com/questions/"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ü–æ–∏—Å–∫ –≤–æ–ø—Ä–æ—Å–æ–≤/—Å—Ç–∞—Ç–µ–π
        items = soup.select('div.question-card, .question-list-item, .forecast-item')[:5]
        
        for item in items:
            title_tag = item.select_one('a.title-link, h3 a, .question-title a')
            if not title_tag:
                continue
                
            title = title_tag.get_text(strip=True)
            link = "https://www.metaculus.com" + title_tag.get('href')
            
            summary_tag = item.select_one('div.blurb, .question-description, p.description')
            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            
            pub_date = datetime.now().isoformat()
            
            if article_exists(link):
                print(f"‚è© –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {title[:50]}...")
                continue
                
            category = classify_article(title, summary)
            if category:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è [{category}]: {title}")
                if save_article(title, link, summary, pub_date, "Metaculus", category):
                    send_to_telegram(title, link, "Metaculus", category, summary)
            else:
                print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é: {title[:50]}...")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Metaculus: {e}")

def scrape_odni():
    print("üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π DNI Global Trends...")
    url = "https://www.dni.gov/index.php/gt2040-home"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π/–æ—Ç—á–µ—Ç–æ–≤
        articles = soup.select('div.article, .press-release, .report-item, .content-item')[:5]
        
        for article in articles:
            title_tag = article.select_one('h2, h3, .title, .headline')
            if not title_tag:
                continue
                
            title = title_tag.get_text(strip=True)
            link_tag = title_tag.find('a')
            if link_tag:
                link = link_tag.get('href')
                if not link.startswith('http'):
                    link = "https://www.dni.gov" + link
            else:
                link = url
            
            summary_tag = article.select_one('p.summary, .description, p')
            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            
            pub_date = datetime.now().isoformat()
            
            if article_exists(link):
                print(f"‚è© –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {title[:50]}...")
                continue
                
            category = classify_article(title, summary)
            if category:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è [{category}]: {title}")
                if save_article(title, link, summary, pub_date, "DNI Global Trends", category):
                    send_to_telegram(title, link, "DNI Global Trends", category, summary)
            else:
                print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é: {title[:50]}...")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ DNI: {e}")

# --- RSS –ø–∞—Ä—Å–µ—Ä—ã ---
def fetch_from_rss(source):
    print(f"üîç –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS: {source['name']}")
    try:
        feed = feedparser.parse(source["rss"])
        if feed.bozo:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {source['name']}: {feed.bozo_exception}")
            return
            
        print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –∏–∑ {source['name']}: {len(feed.entries)}")
        
        for entry in feed.entries[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å–≤–µ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π
            url = entry.link
            title = entry.title.strip()
            summary = entry.get("summary", "") or entry.get("description", "")
            pub_date = entry.get("published", datetime.now().isoformat())
            
            if not url or not title:
                continue
                
            if not is_recent(entry, max_hours=2):
                continue
                
            if article_exists(url):
                print(f"‚è© –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {title[:50]}...")
                continue
                
            category = classify_article(title, summary)
            if category:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è [{category}]: {title}")
                if save_article(title, url, summary, pub_date, source["name"], category):
                    send_to_telegram(title, url, source["name"], category, summary)
            else:
                print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é: {title[:50]}...")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {source['name']}: {e}")

def fetch_and_process():
    for source in SOURCES:
        if "custom_parser" in source:
            print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source['name']} (–∫–∞—Å—Ç–æ–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)")
            source["custom_parser"]()
        else:
            print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source['name']} (RSS)")
            fetch_from_rss(source)

# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ---
if __name__ == "__main__":
    print("üöÄ Background Worker –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ 14 –º–∏–Ω—É—Ç –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏...")
    while True:
        print(f"\n{'='*50}")
        print(f"üïí –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        fetch_and_process()
        print(f"{'='*50}")
        print(f"‚è≥ –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 14 –º–∏–Ω—É—Ç...")
        time.sleep(14 * 60)
