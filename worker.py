import os
import time
import re
import feedparser
import requests
from bs4 import BeautifulSoup
from supabase import create_client
from telegram import Bot
from datetime import datetime
from dateutil import parser as date_parser

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHANNELS = ["@finanosint", "@time_n_John"]

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
bot = Bot(token=TELEGRAM_TOKEN)

# --- –§–∏–ª—å—Ç—Ä—ã ---
FILTERS = {
    "SVO": [
        r"\bsvo\b", r"\b—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è\b", r"\bspecial military operation\b",
        r"\b–≤–æ–π–Ω–∞\b", r"\bwar\b", r"\bconflict\b", r"\b–∫–æ–Ω—Ñ–ª–∏–∫—Ç\b",
        r"\b–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ\b", r"\boffensive\b", r"\b–∞—Ç–∞–∫–∞\b", r"\battack\b",
        r"\b—É–¥–∞—Ä\b", r"\bstrike\b", r"\b–æ–±—Å—Ç—Ä–µ–ª\b", r"\bshelling\b",
        r"\b–¥—Ä–æ–Ω\b", r"\bdrone\b", r"\bmissile\b", r"\b—Ä–∞–∫–µ—Ç–∞\b",
        r"\b—ç—Å–∫–∞–ª–∞—Ü–∏—è\b", r"\bescalation\b", r"\b–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è\b", r"\bmobilization\b",
        r"\b—Ñ—Ä–æ–Ω—Ç\b", r"\bfrontline\b", r"\b–∑–∞—Ö–≤–∞—Ç\b", r"\bcapture\b",
        r"\b–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ\b", r"\bliberation\b", r"\b–±–æ–π\b", r"\bbattle\b",
        r"\b–ø–æ—Ç–µ—Ä–∏\b", r"\bcasualties\b", r"\b–ø–æ–≥–∏–±\b", r"\bkilled\b",
        r"\b—Ä–∞–Ω–µ–Ω\b", r"\binjured\b", r"\b–ø–ª–µ–Ω–Ω—ã–π\b", r"\bprisoner of war\b",
        r"\b–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã\b", r"\btalks\b", r"\b–ø–µ—Ä–µ–º–∏—Ä–∏–µ\b", r"\bceasefire\b",
        r"\b—Å–∞–Ω–∫—Ü–∏–∏\b", r"\bsanctions\b", r"\b–æ—Ä—É–∂–∏–µ\b", r"\bweapons\b",
        r"\b–ø–æ—Å—Ç–∞–≤–∫–∏\b", r"\bsupplies\b", r"\bhimars\b", r"\batacms\b",
        r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bminutos atr√°s\b", r"\bÂ∞èÊó∂Ââç\b"
    ],
    "Crypto": [
        r"\bbitcoin\b", r"\bbtc\b", r"\b–±–∏—Ç–∫–æ–∏–Ω\b", r"\bÊØîÁâπÂ∏Å\b",
        r"\bethereum\b", r"\beth\b", r"\b—ç—Ñ–∏—Ä\b", r"\b‰ª•Â§™Âùä\b",
        r"\bbinance coin\b", r"\bbnb\b", r"\busdt\b", r"\btether\b",
        r"\bxrp\b", r"\bripple\b", r"\bcardano\b", r"\bada\b",
        r"\bsolana\b", r"\bsol\b", r"\bdoge\b", r"\bdogecoin\b",
        r"\bavalanche\b", r"\bavax\b", r"\bpolkadot\b", r"\bdot\b",
        r"\bchainlink\b", r"\blink\b", r"\btron\b", r"\btrx\b",
        r"\bcbdc\b", r"\bcentral bank digital currency\b", r"\b—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å\b",
        r"\bdigital yuan\b", r"\beuro digital\b", r"\bdefi\b", r"\b–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã\b",
        r"\bnft\b", r"\bnon-fungible token\b", r"\bsec\b", r"\b—Ü–± —Ä—Ñ\b",
        r"\b—Ä–µ–≥—É–ª—è—Ü–∏—è\b", r"\bregulation\b", r"\b–∑–∞–ø—Ä–µ—Ç\b", r"\bban\b",
        r"\b–º–∞–π–Ω–∏–Ω–≥\b", r"\bmining\b", r"\bhalving\b", r"\b—Ö–∞–ª–≤–∏–Ω–≥\b",
        r"\b–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å\b", r"\bvolatility\b", r"\bcrash\b", r"\b–∫—Ä–∞—Ö\b",
        r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bÂàöÂàö\b", r"\bÿØŸÇÿßÿ¶ŸÇ ŸÖÿ∂ÿ™\b"
    ],
    "Pandemic": [
        r"\bpandemic\b", r"\b–ø–∞–Ω–¥–µ–º–∏—è\b", r"\bÁñ´ÊÉÖ\b", r"\bÿ¨ÿßÿ¶ÿ≠ÿ©\b",
        r"\boutbreak\b", r"\b–≤—Å–ø—ã—à–∫–∞\b", r"\b—ç–ø–∏–¥–µ–º–∏—è\b", r"\bepidemic\b",
        r"\bvirus\b", r"\b–≤–∏—Ä—É—Å\b", r"\b–≤–∏—Ä—É—Å—ã\b", r"\bÂèòÂºÇÊ†™\b",
        r"\bvaccine\b", r"\b–≤–∞–∫—Ü–∏–Ω–∞\b", r"\bÁñ´Ëãó\b", r"\bŸÑŸÇÿßÿ≠\b",
        r"\bbooster\b", r"\b–±—É—Å—Ç–µ—Ä\b", r"\b—Ä–µ–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è\b",
        r"\bquarantine\b", r"\b–∫–∞—Ä–∞–Ω—Ç–∏–Ω\b", r"\bÈöîÁ¶ª\b", r"\bÿ≠ÿ¨ÿ± ÿµÿ≠Ÿä\b",
        r"\blockdown\b", r"\b–ª–æ–∫–¥–∞—É–Ω\b", r"\bÂ∞ÅÈîÅ\b",
        r"\bmutation\b", r"\b–º—É—Ç–∞—Ü–∏—è\b", r"\bÂèòÂºÇ\b",
        r"\bstrain\b", r"\b—à—Ç–∞–º–º\b", r"\bomicron\b", r"\bdelta\b",
        r"\bbiosafety\b", r"\b–±–∏–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\b", r"\bÁîüÁâ©ÂÆâÂÖ®\b",
        r"\blab leak\b", r"\b–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —É—Ç–µ—á–∫–∞\b", r"\bÂÆûÈ™åÂÆ§Ê≥ÑÊºè\b",
        r"\bgain of function\b", r"\b—É—Å–∏–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\b",
        r"\bwho\b", r"\b–≤–æ–∑\b", r"\bcdc\b", r"\b—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä\b",
        r"\binfection rate\b", r"\b–∑–∞—Ä–∞–∑–Ω–æ—Å—Ç—å\b", r"\bÊ≠ª‰∫°Áéá\b",
        r"\bhospitalization\b", r"\b–≥–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è\b",
        r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bŸÇÿ®ŸÑ ÿ≥ÿßÿπÿßÿ™\b", r"\bÂàöÂàöÊä•Âëä\b"
    ]
}

COMPILED_FILTERS = {
    cat: [re.compile(p, re.IGNORECASE) for p in patterns]
    for cat, patterns in FILTERS.items()
}

# --- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ---
SOURCES = [
    {"name": "The Economist", "rss": "https://www.economist.com/rss/latest/rss.xml"},
    {"name": "Bloomberg", "rss": "https://feeds.bloomberg.com/markets/news.rss"},
    {"name": "RAND Corporation", "rss": "https://www.rand.org/rss.xml"},
    {"name": "CSIS", "rss": "https://www.csis.org/rss.xml"},
    {"name": "Atlantic Council", "rss": "https://www.atlanticcouncil.org/feed/"},
    {"name": "Chatham House", "rss": "https://www.chathamhouse.org/feed"},
    {"name": "Foreign Affairs", "rss": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "CFR", "rss": "https://www.cfr.org/rss.xml"},
    {"name": "BBC Future", "rss": "https://www.bbc.com/future/rss"},
    {"name": "Future Timeline", "rss": "https://futuretimeline.net/blog.rss"},
    {"name": "Carnegie Endowment", "rss": "https://carnegieendowment.org/feed"},
    {"name": "Bruegel", "rss": "https://bruegel.org/feed/"},
    {"name": "E3G", "rss": "https://e3g.org/feed/"},
    {"name": "Good Judgment", "custom_parser": lambda: scrape_good_judgment()},
    {"name": "Metaculus", "custom_parser": lambda: scrape_metaculus()},
    {"name": "DNI Global Trends", "custom_parser": lambda: scrape_odni()},
]

# --- –ü–∞—Ä—Å–µ—Ä—ã ---
def scrape_good_judgment():
    url = "https://goodjudgment.com/blog"
    headers = {"User-Agent": "Mozilla/5.0 (compatible; Bot/1.0)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        posts = soup.find_all('article', class_='post') or soup.find_all('div', class_='blog-post')
        for post in posts:
            title_tag = post.find('h2') or post.find('h3')
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)
            link_tag = title_tag.find('a')
            if not link_tag:
                continue
            link = link_tag.get('href')
            if not link.startswith('http'):
                link = "https://goodjudgment.com" + link
            summary_tag = post.find('p')
            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            pub_date = datetime.now().isoformat()

            if article_exists(link):
                continue
            category = classify_article(title, summary)
            if category:
                save_article(title, link, summary, pub_date, "Good Judgment", category)
                send_to_telegram(title, link, "Good Judgment", category)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Good Judgment: {e}")

def scrape_metaculus():
    url = "https://www.metaculus.com/questions/"
    headers = {"User-Agent": "Mozilla/5.0 (compatible; Bot/1.0)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.find_all('div', class_='question-card') or soup.find_all('div', class_='question-list-item')
        for item in items:
            title_tag = item.find('a', class_='title-link') or item.find('h3').find('a')
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)
            link = "https://www.metaculus.com" + title_tag.get('href')
            summary_tag = item.find('div', class_='blurb') or item.find('p')
            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            pub_date = datetime.now().isoformat()

            if article_exists(link):
                continue
            category = classify_article(title, summary)
            if category:
                save_article(title, link, summary, pub_date, "Metaculus", category)
                send_to_telegram(title, link, "Metaculus", category)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Metaculus: {e}")

def scrape_odni():
    url = "https://www.dni.gov/index.php/gt2040-home"
    headers = {"User-Agent": "Mozilla/5.0 (compatible; Bot/1.0)"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('div', class_='article') or soup.find_all('div', class_='press-release')
        for article in articles:
            title_tag = article.find('h3') or article.find('h2')
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)
            link_tag = title_tag.find('a')
            if link_tag:
                link = link_tag.get('href')
                if not link.startswith('http'):
                    link = "https://www.dni.gov" + link
            else:
                link = url
            summary_tag = article.find('p')
            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            pub_date = datetime.now().isoformat()

            if article_exists(link):
                continue
            category = classify_article(title, summary)
            if category:
                save_article(title, link, summary, pub_date, "DNI Global Trends", category)
                send_to_telegram(title, link, "DNI Global Trends", category)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ DNI: {e}")

# --- –§—É–Ω–∫—Ü–∏–∏ ---
def contains_keywords(text, category):
    if not text:
        return False
    return any(pattern.search(text) for pattern in COMPILED_FILTERS[category])

def classify_article(title, summary):
    text = f"{title} {summary}".lower()
    for category in ["SVO", "Crypto", "Pandemic"]:
        if contains_keywords(text, category):
            return category
    return None

def is_recent(entry, max_hours=2):
    try:
        pub = date_parser.parse(entry.published)
        now = datetime.now(pub.tzinfo)
        diff_hours = (now - pub).total_seconds() / 3600
        return diff_hours < max_hours
    except:
        return True

def article_exists(url):
    response = supabase.table("news_articles").select("id").eq("url", url).execute()
    return len(response.data) > 0

def save_article(title, url, description, pub_date, source, category):
    supabase.table("news_articles").insert({
        "title": title,
        "url": url,
        "description": description or "",
        "pub_date": pub_date,
        "source_name": source,
        "category": category
    }).execute()

def send_to_telegram(title, url, source, category):
    message = (
        f"[{category}] {title}\n"
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: <a href='{url}'>{source}</a>"
    )
    for channel in CHANNELS:
        try:
            bot.send_message(chat_id=channel, text=message, parse_mode="HTML", disable_web_page_preview=False)
            print(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {channel}: {title}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {channel}: {e}")

# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ---
def fetch_from_rss(source):
    try:
        feed = feedparser.parse(source["rss"])
        if feed.bozo:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {source['name']}: {feed.bozo_exception}")
            return
        for entry in feed.entries:
            url = entry.link
            title = entry.title.strip()
            summary = entry.get("summary", "") or entry.get("description", "")
            pub_date = entry.get("published", datetime.now().isoformat())

            if not url or not title:
                continue

            if not is_recent(entry, max_hours=2):
                continue

            if article_exists(url):
                continue

            category = classify_article(title, summary)
            if category:
                save_article(title, url, summary, pub_date, source["name"], category)
                send_to_telegram(title, url, source["name"], category)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {source['name']}: {e}")

def fetch_and_process():
    for source in SOURCES:
        if "custom_parser" in source:
            source["custom_parser"]()
        else:
            fetch_from_rss(source)

if __name__ == "__main__":
    print("üöÄ Background Worker –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ 14 –º–∏–Ω—É—Ç –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏...")
    while True:
        print(f"\nüïí –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        fetch_and_process()
        print(f"‚è≥ –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 14 –º–∏–Ω—É—Ç...")
        time.sleep(14 * 60)
